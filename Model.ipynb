{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feat_names = [\"workclass\", \"education_level\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\\\n",
    "                  \"native-country\"]\n",
    "num_feat_names = [\"age\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, csv_file, cat_feats, num_feats):\n",
    "        self.cat_feats = cat_feats\n",
    "        self.num_feats = num_feats\n",
    "        self.num_of_records = None\n",
    "        self.features = None\n",
    "        self.target= None\n",
    "        self.parse_file(csv_file)\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.preprocess()\n",
    "        self.split_train_test()\n",
    "    \n",
    "    def parse_file(self, csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        self.features = df[self.cat_feats + self.num_feats].copy()\n",
    "        self.target = np.reshape(df[\"income\"].copy().values, (-1,1))\n",
    "        self.num_of_records = self.features.shape[0]\n",
    "    \n",
    "    def log_transformation(self, feature):\n",
    "        self.features[feature] = self.features[feature].apply(lambda x: np.log(x+1))\n",
    "    \n",
    "    def scale_data(self):\n",
    "        num_values = self.features[self.num_feats].copy()\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_values = scaler.fit_transform(num_values)\n",
    "        scaled_df = pd.DataFrame(scaled_values, columns=self.num_feats)\n",
    "        output_df = pd.concat([scaled_df, self.features[self.cat_feats]], axis=1)\n",
    "        self.features = output_df\n",
    "    \n",
    "    def encode_categorical_features(self):\n",
    "        encoded_df = pd.get_dummies(self.features[self.cat_feats])\n",
    "        num_vars = self.features[self.num_feats]\n",
    "        self.features = pd.concat([num_vars, encoded_df], axis=1)\n",
    "        \n",
    "    def encode_target(self):\n",
    "        target_df = pd.DataFrame(self.target, columns=[\"income\"])\n",
    "        self.target = target_df[\"income\"].map({\"<=50K\":0, \">50K\":1})\n",
    "    \n",
    "    def preprocess(self):\n",
    "        self.log_transformation(\"capital-loss\")\n",
    "        self.log_transformation(\"capital-gain\")\n",
    "        self.scale_data()\n",
    "        self.encode_categorical_features()\n",
    "        self.encode_target()\n",
    "        \n",
    "    def split_train_test(self):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "        train_test_split(self.features, self.target, test_size=0.2, random_state = 0)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Selection():\n",
    "    def __init__(self, data_, k=5):\n",
    "        self.data = data_\n",
    "        self.num_folds = k\n",
    "        self.best_model = None\n",
    "        self.models = list()\n",
    "        self.report = dict()\n",
    "        self.validation_scores = dict()\n",
    "        self.fbeta_scores = dict()\n",
    "#         self.sample_sizes = sample_sizes_\n",
    "    \n",
    "    def add_model(self, model_):\n",
    "        self.models.append(model_)\n",
    "            \n",
    "    def cross_validate(self, k):\n",
    "        scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "        for model in self.models:\n",
    "            val_score = dict()\n",
    "            cross_val_X = pd.concat([self.data.X_train, self.data.X_test])\n",
    "            cross_val_y = pd.concat([self.data.y_train, self.data.y_test])\n",
    "            cross_val_results = cross_val_score(model, cross_val_X, cross_val_y, cv=k, scoring=scorer)\n",
    "            val_score[\"fscore_mean\"] = np.mean(cross_val_results)\n",
    "            val_score[\"fscore_std\"] = np.std(cross_val_results)\n",
    "            \n",
    "            model.fit(self.data.X_train, self.data.y_train)\n",
    "            train_predictions = model.predict(self.data.X_train)\n",
    "            test_predictions = model.predict(self.data.X_test)   \n",
    "            train_fscore = fbeta_score(self.data.y_train, train_predictions, beta=0.5)\n",
    "            test_fscore = fbeta_score(self.data.y_test, test_predictions, beta=0.5)\n",
    "            val_score[\"fscore_train\"] = train_fscore\n",
    "            \n",
    "            self.fbeta_scores[model] = test_fscore\n",
    "            \n",
    "            self.validation_scores[model.__class__.__name__] = val_score\n",
    "            \n",
    "    def summarize_validation(self):\n",
    "        for model_ in self.models:\n",
    "            model_name = model_.__class__.__name__\n",
    "            print(\"{}\".format(model_name))\n",
    "            model_validation_results = self.validation_scores[model_name]\n",
    "            print(\"Mean F-beta score: {}\".format(model_validation_results[\"fscore_mean\"]))\n",
    "            print(\"Standard Deviation of F-beta score: {}\".format(model_validation_results[\"fscore_std\"]))\n",
    "            print(\"Training Set F-beta score: {}\".format(model_validation_results[\"fscore_train\"]))\n",
    "            print(\"Test Set F-beta score: {}\\n\".format(self.fbeta_scores[model_]))\n",
    "            \n",
    "    def run(self):\n",
    "        self.cross_validate(self.num_folds)\n",
    "        self.summarize_validation()\n",
    "        self.find_best_fit()\n",
    "            \n",
    "    def find_best_fit(self):\n",
    "        self.best_model = max(self.fbeta_scores, key=self.fbeta_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Mean F-beta score: 0.6889350945662823\n",
      "Standard Deviation of F-beta score: 0.00355836285111309\n",
      "Training Set F-beta score: 0.6944049985799489\n",
      "Test Set F-beta score: 0.6831652282416554\n",
      "\n",
      "GradientBoostingClassifier\n",
      "Mean F-beta score: 0.7455970908414289\n",
      "Standard Deviation of F-beta score: 0.0055289268803982985\n",
      "Training Set F-beta score: 0.7530470502391698\n",
      "Test Set F-beta score: 0.7395338561802719\n",
      "\n",
      "RandomForestClassifier\n",
      "Mean F-beta score: 0.672867765310291\n",
      "Standard Deviation of F-beta score: 0.0061673040160426865\n",
      "Training Set F-beta score: 0.935004513575446\n",
      "Test Set F-beta score: 0.678261736270308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    logistic_regression_model = LogisticRegression()\n",
    "    gradient_boosting_model = GradientBoostingClassifier()\n",
    "    random_forest_model = RandomForestClassifier()\n",
    "    \n",
    "    data_ = Data(\"census.csv\",cat_feat_names, num_feat_names)\n",
    "    \n",
    "    model_selector = Model_Selection(data_)\n",
    "    model_selector.add_model(logistic_regression_model)\n",
    "    model_selector.add_model(gradient_boosting_model)\n",
    "    model_selector.add_model(random_forest_model)\n",
    "    model_selector.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier\n"
     ]
    }
   ],
   "source": [
    "model_selector.find_best_fit()\n",
    "print(model_selector.best_model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
